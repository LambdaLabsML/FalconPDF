---
sdk: gradio
sdk_version: 3.36.1
app_file: app.py
---

# Chat with PDF using Falcon


This demo allows you to upload a PDF and ask questions about its contents. Falcon will read the PDF and try to answer your questions.

## Key Features
* Upload PDFs to use as context for questions
* Use Falcon models (Falcon-7B or Falcon-40B) to understand documents
* Choose between basic Q&A mode or conversational mode with dialog history
* Ask questions naturally and get answers generated by Falcon

## Usage
This project can be directly used or as a demo on Lambda Cloud.

Start the app:
```
python run.py
```
Then, open the link in your browser.

1. Upload a PDF file.
2. Select a Falcon model
3. Choose between basic or conversational mode
4. Type a question in the input box and submit to get Falcon's answer

### Examples
See the example questions for ideas. Try asking "What is the summary of the document?" as a starting point.

## Technical details
### Overview
The PDFs, or other documents, serve as the context for question answering. In this project, the context is stored using the Chroma database. While simple files like .txt can be loaded directly, PDF files need to be parsed using tools designed for unstructured data. The documents are then split into chunks, which are used for subsequent retrieval. When a user asks a question, the embedding of the question is calculated and compared with the embeddings of the chunks. The most similar chunk is selected, and the question is fed into the LLM (Language Model) model to obtain the answer.

![img_1.png](img.png)

### Falcon models
This project utilizes the `Falocn-7b-instruct` and `Falcon-40b-instruct` models, which have been fine-tuned on instruct datasets.

#### Stopping criteria
To prevent the model from generating an infinite loop, a stopping criteria called `StopOnWords` is used. This criteria stops the generation process when the generated text contains one of the words in the predefined list. Please note that the stopping words can be affected by the system prompt. For example, a simple AI/User conversation may use `AI:` and `User:` as stopping words, while for question answering tasks, the stopping words may be `Question:` and `Answer:`.

#### Randomness
The randomness in the output is a result of both the inherent randomness of the model and the sampling process.

#### Repetition penalty
In some cases, the model may repeat a sentence indefinitely. To address this issue, a repetition penalty is applied. Increasing the value of the repetition penalty reduces the likelihood of the model repeating a sentence.

### Embedding
This project uses the sentence-transformers library (MPNet) to calculate the embeddings of the questions and chunks. Sentence-transformers is a library for computing sentence embeddings, which are then compared using cosine similarity to find sentences with similar meanings. There are flexible options available for the embedding calculation.

### Langchain retrieval-based question answering

#### Basic retrieval
In the Langchain framework, basic retrieval-based question answering works as described above. In each round, the user asks a clear question, and the model responds with an answer. It's important to note that this mode does not consider conversation history or support multi-round interactions.

#### Conversational retrieval
Conversational retrieval-based question answering is similar to basic retrieval-based question answering, but with the added consideration of conversation history. This enables more complex conversations like the following example:

```
(Given the PDF document about a government policy)

User: What is the motivation of the document?
Bot: The motivation of the document is to provide information to the government about the application and to help them make a decision on whether to grant the visa or not.

User: What affects the decision?
Bot: The decision is based on the information provided in the application form and the supporting documents. The applicant must provide all the necessary information and documents to enable UK Visas and Immigration to make a decision. If the applicant does not provide all the necessary information and documents, the application may be refused.
```

In the second question, the term "decision" refers to the "decision" mentioned in the first question. This is possible because the model takes the conversation history into consideration.

#### Which mode to choose?
- Basic mode: Use this mode when you can provide a clear and simple question that is not related to the conversation history. For example, questions without pronouns.
- Conversational mode: Choose this mode when you want to ask questions based on the conversation history or when you prefer a more natural way of asking questions. It's important to note that conversational mode is not always superior to basic mode. In some cases, the model may be more influenced by the conversation history than the current question, especially when the user asks a new question.

## TODO
Some future work:
- [ ] Implement streaming for a more natural user experience.
- [ ] Add more advanced options, such as different embedding methods.
- [ ] Explore powerful PDF indexing tools.


## Credits
Some of the work are based on the following projects:
- Falcon: https://falconllm.tii.ae/
- https://github.com/camenduru/falcon-40b-instruct-lambda